{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CawfacZ_N6Ic"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configuration file for Financial Metrics Extraction Pipeline\n",
        "\"\"\"\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class APIConfig:\n",
        "    \"\"\"API Configuration\"\"\"\n",
        "    newsapi_key: str = os.getenv('NEWSAPI_KEY', '')\n",
        "    alpha_vantage_key: str = os.getenv('ALPHA_VANTAGE_KEY', '')\n",
        "    finnhub_key: str = os.getenv('FINNHUB_KEY', '')\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    \"\"\"Data Collection Configuration\"\"\"\n",
        "    max_articles_per_company: int = 100\n",
        "    companies: List[str] = field(default_factory=lambda: [\n",
        "        \"Apple\", \"Microsoft\", \"Google\", \"Amazon\", \"Tesla\", \"Meta\", \"Netflix\",\n",
        "        \"Nvidia\", \"JPMorgan\", \"Goldman Sachs\", \"Boeing\", \"Coca Cola\", \"Disney\",\n",
        "        \"McDonald's\", \"Nike\", \"Walmart\", \"Visa\", \"Intel\", \"Adobe\", \"Salesforce\"\n",
        "    ])\n",
        "    news_sources: List[str] = field(default_factory=lambda: [\n",
        "        \"reuters\", \"bloomberg\", \"financial-times\", \"wall-street-journal\",\n",
        "        \"cnbc\", \"business-insider\", \"fortune\", \"marketwatch\"\n",
        "    ])\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model Configuration\"\"\"\n",
        "    model_name: str = \"microsoft/DialoGPT-medium\"\n",
        "    max_length: int = 512\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 5e-5\n",
        "    num_epochs: int = 5\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.01\n",
        "    save_steps: int = 500\n",
        "    eval_steps: int = 500\n",
        "    save_total_limit: int = 2\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training Configuration\"\"\"\n",
        "    train_split: float = 0.8\n",
        "    val_split: float = 0.1\n",
        "    test_split: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "@dataclass\n",
        "class PathConfig:\n",
        "    \"\"\"Path Configuration\"\"\"\n",
        "    data_dir: str = \"data\"\n",
        "    model_dir: str = \"models\"\n",
        "    output_dir: str = \"outputs\"\n",
        "    raw_data_dir: str = \"data/raw\"\n",
        "    processed_data_dir: str = \"data/processed\"\n",
        "    model_checkpoints_dir: str = \"models/checkpoints\"\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Main Configuration Class\"\"\"\n",
        "    api: APIConfig = field(default_factory=APIConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
        "    paths: PathConfig = field(default_factory=PathConfig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEnzZH0GPWG7",
        "outputId": "fa861813-385e-4a03-966c-635f13497ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from newsapi-python) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2025.4.26)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install newsapi-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MtCHzy7PfVe",
        "outputId": "e2c8c5c4-9fdb-4ea3-f087-e3caeade52de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting finnhub-python\n",
            "  Downloading finnhub_python-2.4.23-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from finnhub-python) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->finnhub-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->finnhub-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->finnhub-python) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->finnhub-python) (2025.4.26)\n",
            "Downloading finnhub_python-2.4.23-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: finnhub-python\n",
            "Successfully installed finnhub-python-2.4.23\n"
          ]
        }
      ],
      "source": [
        "!pip install finnhub-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8E7zaNpUIVA",
        "outputId": "c0090283-7bcd-489e-f013-b202180a9c86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:NewsAPI test failed: type object 'Config' has no attribute 'api'\n",
            "ERROR:__main__:Please check your NewsAPI key in config/config.py\n",
            "ERROR:__main__:API key validation failed. Using synthetic data fallback.\n"
          ]
        }
      ],
      "source": [
        "def test_api_keys(config):\n",
        "    \"\"\"Test API keys before starting pipeline\"\"\"\n",
        "    from newsapi import NewsApiClient\n",
        "\n",
        "    try:\n",
        "        newsapi = NewsApiClient(api_key=config.api.newsapi_key)\n",
        "        test_result = newsapi.get_everything(q=\"Apple\", page_size=1)\n",
        "        logger.info(f\"NewsAPI test successful: {test_result['totalResults']} results available\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"NewsAPI test failed: {str(e)}\")\n",
        "        logger.error(\"Please check your NewsAPI key in config/config.py\")\n",
        "        return False\n",
        "\n",
        "# Add this call in main() before data collection:\n",
        "if not test_api_keys(Config):\n",
        "    logger.error(\"API key validation failed. Using synthetic data fallback.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8Khw1BwNN-Bk"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data collection module for financial news and metrics\n",
        "\"\"\"\n",
        "import requests\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from newsapi import NewsApiClient\n",
        "import finnhub\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FinancialDataCollector:\n",
        "    \"\"\"Collect financial news and market data\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.newsapi = NewsApiClient(api_key=config.api.newsapi_key)\n",
        "        self.finnhub_client = finnhub.Client(api_key=config.api.finnhub_key)\n",
        "\n",
        "    def collect_news_data(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Collect financial news articles with fallback strategies\"\"\"\n",
        "        all_articles = []\n",
        "\n",
        "        for company in self.config.data.companies:\n",
        "            logger.info(f\"Collecting news for {company}\")\n",
        "\n",
        "            try:\n",
        "                # Primary strategy: Specific financial keywords\n",
        "                articles = self.newsapi.get_everything(\n",
        "                    q=f\"{company} AND (earnings OR revenue OR profit OR financial)\",\n",
        "                    language='en',\n",
        "                    sort_by='relevancy',\n",
        "                    page_size=min(100, self.config.data.max_articles_per_company),\n",
        "                    from_param=(datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "                )\n",
        "\n",
        "                if articles['totalResults'] == 0:\n",
        "                    logger.warning(f\"No articles found for {company} with financial keywords. Trying broader search...\")\n",
        "\n",
        "                    # Fallback strategy: Broader search\n",
        "                    articles = self.newsapi.get_everything(\n",
        "                        q=company,\n",
        "                        language='en',\n",
        "                        sort_by='relevancy',\n",
        "                        page_size=min(50, self.config.data.max_articles_per_company),\n",
        "                        from_param=(datetime.now() - timedelta(days=60)).strftime('%Y-%m-%d')\n",
        "                    )\n",
        "\n",
        "                logger.info(f\"Found {articles['totalResults']} articles for {company}\")\n",
        "\n",
        "                for article in articles['articles']:\n",
        "                    if article['content'] and len(article['content']) > 100:\n",
        "                        all_articles.append({\n",
        "                            'company': company,\n",
        "                            'title': article['title'],\n",
        "                            'content': article['content'],\n",
        "                            'source': article['source']['name'],\n",
        "                            'published_at': article['publishedAt'],\n",
        "                            'url': article['url']\n",
        "                        })\n",
        "\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"NewsAPI error for {company}: {str(e)}\")\n",
        "\n",
        "                # Fallback: Generate synthetic news if API fails\n",
        "                logger.info(f\"Generating synthetic news for {company}\")\n",
        "                synthetic_article = self.generate_synthetic_news(company)\n",
        "                all_articles.append(synthetic_article)\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Collected {len(all_articles)} articles total\")\n",
        "\n",
        "        # Ensure minimum data\n",
        "        if len(all_articles) < 20:\n",
        "            logger.warning(\"Insufficient articles collected. Generating additional synthetic data...\")\n",
        "            additional_needed = 20 - len(all_articles)\n",
        "            for i in range(additional_needed):\n",
        "                company = self.config.data.companies[i % len(self.config.data.companies)]\n",
        "                synthetic_article = self.generate_synthetic_news(company)\n",
        "                all_articles.append(synthetic_article)\n",
        "\n",
        "        return all_articles\n",
        "\n",
        "    def generate_synthetic_news(self, company: str) -> Dict[str, Any]:\n",
        "        \"\"\"Generate synthetic news article for training\"\"\"\n",
        "        templates = [\n",
        "            f\"{company} Reports Strong Q3 2024 Financial Results with Record Revenue Growth\",\n",
        "            f\"{company} Announces Quarterly Earnings Beat with Improved Profit Margins\",\n",
        "            f\"{company} Delivers Solid Financial Performance Despite Market Challenges\",\n",
        "            f\"{company} Posts Strong Revenue Growth and Expanded Operating Margins\"\n",
        "        ]\n",
        "\n",
        "        content_templates = [\n",
        "            f\"{company} today announced financial results for the third quarter ended September 30, 2024. The company reported revenue of ${{revenue}} million, representing a {{growth}}% increase year-over-year. Net income was ${{net_income}} million with a profit margin of {{margin}}%. The strong results were driven by increased demand and operational efficiency improvements.\",\n",
        "\n",
        "            f\"{company} delivered strong financial performance in Q3 2024 with total revenue of ${{revenue}} million and operating margin of {{operating_margin}}%. The company's net income reached ${{net_income}} million, reflecting effective cost management and strong market position.\",\n",
        "        ]\n",
        "\n",
        "        # Generate random but realistic financial numbers\n",
        "        revenue = np.random.uniform(5000, 100000)\n",
        "        growth = np.random.uniform(3, 25)\n",
        "        net_income = revenue * np.random.uniform(0.05, 0.25)\n",
        "        margin = (net_income / revenue) * 100\n",
        "        operating_margin = np.random.uniform(10, 30)\n",
        "\n",
        "        title = np.random.choice(templates)\n",
        "        content = np.random.choice(content_templates).format(\n",
        "            revenue=f\"{revenue:,.0f}\",\n",
        "            growth=f\"{growth:.1f}\",\n",
        "            net_income=f\"{net_income:,.0f}\",\n",
        "            margin=f\"{margin:.1f}\",\n",
        "            operating_margin=f\"{operating_margin:.1f}\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'company': company,\n",
        "            'title': title,\n",
        "            'content': content,\n",
        "            'source': 'Synthetic',\n",
        "            'published_at': datetime.now().isoformat(),\n",
        "            'url': 'synthetic://example.com'\n",
        "        }\n",
        "\n",
        "        def collect_financial_metrics(self) -> Dict[str, Dict[str, Any]]:\n",
        "            \"\"\"Collect actual financial metrics from Yahoo Finance\"\"\"\n",
        "            financial_data = {}\n",
        "\n",
        "            # Convert company names to tickers\n",
        "            ticker_mapping = {\n",
        "                \"Apple\": \"AAPL\", \"Microsoft\": \"MSFT\", \"Google\": \"GOOGL\",\n",
        "                \"Amazon\": \"AMZN\", \"Tesla\": \"TSLA\", \"Meta\": \"META\",\n",
        "                \"Netflix\": \"NFLX\", \"Nvidia\": \"NVDA\", \"JPMorgan\": \"JPM\",\n",
        "                \"Goldman Sachs\": \"GS\", \"Boeing\": \"BA\", \"Coca Cola\": \"KO\",\n",
        "                \"Disney\": \"DIS\", \"McDonald's\": \"MCD\", \"Nike\": \"NKE\",\n",
        "                \"Walmart\": \"WMT\", \"Visa\": \"V\", \"Intel\": \"INTC\",\n",
        "                \"Adobe\": \"ADBE\", \"Salesforce\": \"CRM\"\n",
        "            }\n",
        "\n",
        "            for company, ticker in ticker_mapping.items():\n",
        "                try:\n",
        "                    print(f\"Collecting financial data for {company} ({ticker})\")\n",
        "\n",
        "                    stock = yf.Ticker(ticker)\n",
        "                    info = stock.info\n",
        "                    financials = stock.financials\n",
        "\n",
        "                    # Extract key metrics\n",
        "                    metrics = {\n",
        "                        'revenue': info.get('totalRevenue', 0),\n",
        "                        'profit_margin': info.get('profitMargins', 0) * 100 if info.get('profitMargins') else 0,\n",
        "                        'gross_margin': info.get('grossMargins', 0) * 100 if info.get('grossMargins') else 0,\n",
        "                        'operating_margin': info.get('operatingMargins', 0) * 100 if info.get('operatingMargins') else 0,\n",
        "                        'net_income': info.get('netIncomeToCommon', 0),\n",
        "                        'total_debt': info.get('totalDebt', 0),\n",
        "                        'market_cap': info.get('marketCap', 0),\n",
        "                        'pe_ratio': info.get('trailingPE', 0),\n",
        "                        'eps': info.get('trailingEps', 0),\n",
        "                        'book_value': info.get('bookValue', 0),\n",
        "                        'debt_to_equity': info.get('debtToEquity', 0),\n",
        "                        'roe': info.get('returnOnEquity', 0) * 100 if info.get('returnOnEquity') else 0,\n",
        "                        'roa': info.get('returnOnAssets', 0) * 100 if info.get('returnOnAssets') else 0\n",
        "                    }\n",
        "\n",
        "                    financial_data[company] = metrics\n",
        "                    time.sleep(1)  # Rate limiting\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error collecting financial data for {company}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            return financial_data\n",
        "\n",
        "    def save_raw_data(self, news_data: List[Dict], financial_data: Dict):\n",
        "        \"\"\"Save raw collected data\"\"\"\n",
        "        os.makedirs(self.config.paths.raw_data_dir, exist_ok=True)\n",
        "\n",
        "        # Save news data\n",
        "        news_df = pd.DataFrame(news_data)\n",
        "        news_df.to_csv(f\"{self.config.paths.raw_data_dir}/news_data.csv\", index=False)\n",
        "\n",
        "        # Save financial data\n",
        "        financial_df = pd.DataFrame.from_dict(financial_data, orient='index')\n",
        "        financial_df.to_csv(f\"{self.config.paths.raw_data_dir}/financial_data.csv\")\n",
        "\n",
        "        print(\"Raw data saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xFpu0SeRPNk3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data preprocessing module for training data preparation\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import json\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FinancialDataPreprocessor:\n",
        "    \"\"\"Preprocess financial data for training\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # Financial metrics patterns\n",
        "        self.metric_patterns = {\n",
        "            'revenue': [\n",
        "                r'revenue of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?',\n",
        "                r'sales of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?',\n",
        "                r'total revenue \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?'\n",
        "            ],\n",
        "            'profit_margin': [\n",
        "                r'profit margin of ([\\d,]+\\.?\\d*)%?',\n",
        "                r'operating margin of ([\\d,]+\\.?\\d*)%?',\n",
        "                r'net margin of ([\\d,]+\\.?\\d*)%?'\n",
        "            ],\n",
        "            'net_income': [\n",
        "                r'net income of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?',\n",
        "                r'profit of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?',\n",
        "                r'earnings of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?'\n",
        "            ],\n",
        "            'market_cap': [\n",
        "                r'market cap of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?',\n",
        "                r'market capitalization of \\$?([\\d,]+\\.?\\d*)\\s*(million|billion|thousand)?'\n",
        "            ],\n",
        "            'pe_ratio': [\n",
        "                r'P/E ratio of ([\\d,]+\\.?\\d*)',\n",
        "                r'price-to-earnings ratio of ([\\d,]+\\.?\\d*)',\n",
        "                r'PE of ([\\d,]+\\.?\\d*)'\n",
        "            ],\n",
        "            'eps': [\n",
        "                r'earnings per share of \\$?([\\d,]+\\.?\\d*)',\n",
        "                r'EPS of \\$?([\\d,]+\\.?\\d*)'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def load_raw_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Load raw collected data\"\"\"\n",
        "        news_df = pd.read_csv(f\"{self.config.paths.raw_data_dir}/news_data.csv\")\n",
        "        financial_df = pd.read_csv(f\"{self.config.paths.raw_data_dir}/financial_data.csv\", index_col=0)\n",
        "\n",
        "        print(f\"Loaded {len(news_df)} news articles and {len(financial_df)} financial records\")\n",
        "        return news_df, financial_df\n",
        "\n",
        "    def extract_metrics_from_text(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract financial metrics from text using regex patterns\"\"\"\n",
        "        extracted_metrics = {}\n",
        "\n",
        "        for metric_type, patterns in self.metric_patterns.items():\n",
        "            extracted_metrics[metric_type] = []\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.findall(pattern, text.lower())\n",
        "                for match in matches:\n",
        "                    if isinstance(match, tuple):\n",
        "                        value = match[0]\n",
        "                        unit = match[1] if len(match) > 1 else ''\n",
        "                    else:\n",
        "                        value = match\n",
        "                        unit = ''\n",
        "\n",
        "                    # Normalize value\n",
        "                    normalized_value = self.normalize_financial_value(value, unit)\n",
        "                    if normalized_value:\n",
        "                        extracted_metrics[metric_type].append(normalized_value)\n",
        "\n",
        "        return extracted_metrics\n",
        "\n",
        "    def normalize_financial_value(self, value: str, unit: str) -> str:\n",
        "        \"\"\"Normalize financial values to consistent format\"\"\"\n",
        "        try:\n",
        "            # Remove commas and convert to float\n",
        "            numeric_value = float(value.replace(',', ''))\n",
        "\n",
        "            # Convert based on unit\n",
        "            if unit.lower() == 'billion':\n",
        "                numeric_value *= 1000000000\n",
        "            elif unit.lower() == 'million':\n",
        "                numeric_value *= 1000000\n",
        "            elif unit.lower() == 'thousand':\n",
        "                numeric_value *= 1000\n",
        "\n",
        "            return str(numeric_value)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def create_training_examples(self, news_df: pd.DataFrame, financial_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Create training examples by combining news text with actual financial metrics\"\"\"\n",
        "        training_examples = []\n",
        "\n",
        "        for _, row in news_df.iterrows():\n",
        "            company = row['company']\n",
        "            text = f\"{row['title']} {row['content']}\"\n",
        "\n",
        "            # Get actual financial metrics for this company\n",
        "            if company in financial_df.index:\n",
        "                actual_metrics = financial_df.loc[company].to_dict()\n",
        "\n",
        "                # Extract mentioned metrics from text\n",
        "                extracted_metrics = self.extract_metrics_from_text(text)\n",
        "\n",
        "                # Create training example\n",
        "                training_example = {\n",
        "                    'input_text': text,\n",
        "                    'company': company,\n",
        "                    'actual_metrics': actual_metrics,\n",
        "                    'extracted_metrics': extracted_metrics,\n",
        "                    'target_output': self.format_metrics_as_json(actual_metrics)\n",
        "                }\n",
        "\n",
        "                training_examples.append(training_example)\n",
        "\n",
        "        print(f\"Created {len(training_examples)} training examples\")\n",
        "        return training_examples\n",
        "\n",
        "    def format_metrics_as_json(self, metrics: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format metrics as JSON string for training target\"\"\"\n",
        "        formatted_metrics = {}\n",
        "\n",
        "        for key, value in metrics.items():\n",
        "            if pd.notna(value) and value != 0:\n",
        "                if key in ['revenue', 'net_income', 'total_debt', 'market_cap']:\n",
        "                    formatted_metrics[key] = f\"${value:,.0f}\"\n",
        "                elif key in ['profit_margin', 'gross_margin', 'operating_margin', 'roe', 'roa']:\n",
        "                    formatted_metrics[key] = f\"{value:.2f}%\"\n",
        "                elif key in ['pe_ratio', 'eps', 'book_value', 'debt_to_equity']:\n",
        "                    formatted_metrics[key] = f\"{value:.2f}\"\n",
        "\n",
        "        return json.dumps(formatted_metrics)\n",
        "\n",
        "    def create_conversational_format(self, training_examples: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
        "        \"\"\"Convert training examples to conversational format\"\"\"\n",
        "        conversational_data = []\n",
        "\n",
        "        for example in training_examples:\n",
        "            # Create instruction-following format\n",
        "            instruction = f\"Extract financial metrics from the following text about {example['company']}:\"\n",
        "            user_message = f\"{instruction}\\n\\n{example['input_text']}\"\n",
        "            assistant_message = example['target_output']\n",
        "\n",
        "            conversational_data.append({\n",
        "                'user': user_message,\n",
        "                'assistant': assistant_message\n",
        "            })\n",
        "\n",
        "        return conversational_data\n",
        "\n",
        "    def split_and_save_data(self, conversational_data: List[Dict[str, str]]):\n",
        "        \"\"\"Split data and save to files with minimum data validation\"\"\"\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(conversational_data)\n",
        "\n",
        "        # Check minimum data requirements\n",
        "        min_samples = 10  # Minimum samples needed for meaningful training\n",
        "\n",
        "        if len(df) < min_samples:\n",
        "            logger.warning(f\"Only {len(df)} samples collected. Minimum {min_samples} needed.\")\n",
        "            logger.warning(\"Generating synthetic examples to supplement data...\")\n",
        "\n",
        "            # Generate synthetic examples\n",
        "            synthetic_data = self.generate_synthetic_examples(df, min_samples - len(df))\n",
        "            df = pd.concat([df, pd.DataFrame(synthetic_data)], ignore_index=True)\n",
        "            logger.info(f\"Generated {len(synthetic_data)} synthetic examples\")\n",
        "\n",
        "        # Adjusted splitting for small datasets\n",
        "        if len(df) < 50:\n",
        "            # For very small datasets, use different split ratios\n",
        "            train_split = 0.7\n",
        "            val_split = 0.2\n",
        "            test_split = 0.1\n",
        "            logger.warning(f\"Small dataset detected ({len(df)} samples). Using adjusted splits: 70/20/10\")\n",
        "        else:\n",
        "            train_split = self.config.training.train_split\n",
        "            val_split = self.config.training.val_split\n",
        "            test_split = self.config.training.test_split\n",
        "\n",
        "        # Ensure minimum 1 sample in each split\n",
        "        n_samples = len(df)\n",
        "        n_test = max(1, int(n_samples * test_split))\n",
        "        n_val = max(1, int(n_samples * val_split))\n",
        "        n_train = n_samples - n_test - n_val\n",
        "\n",
        "        if n_train < 1:\n",
        "            n_train = 1\n",
        "            n_val = max(1, (n_samples - n_train) // 2)\n",
        "            n_test = n_samples - n_train - n_val\n",
        "\n",
        "        logger.info(f\"Data split: Train={n_train}, Val={n_val}, Test={n_test}\")\n",
        "\n",
        "        # Manual splitting for small datasets\n",
        "        df_shuffled = df.sample(frac=1, random_state=self.config.training.seed).reset_index(drop=True)\n",
        "\n",
        "        train_data = df_shuffled[:n_train]\n",
        "        val_data = df_shuffled[n_train:n_train + n_val]\n",
        "        test_data = df_shuffled[n_train + n_val:]\n",
        "\n",
        "        # Save data\n",
        "        os.makedirs(self.config.paths.processed_data_dir, exist_ok=True)\n",
        "\n",
        "        train_data.to_json(f\"{self.config.paths.processed_data_dir}/train_data.json\", orient='records', lines=True)\n",
        "        val_data.to_json(f\"{self.config.paths.processed_data_dir}/val_data.json\", orient='records', lines=True)\n",
        "        test_data.to_json(f\"{self.config.paths.processed_data_dir}/test_data.json\", orient='records', lines=True)\n",
        "\n",
        "        logger.info(f\"Data split and saved - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    def generate_synthetic_examples(self, original_df: pd.DataFrame, num_needed: int) -> List[Dict[str, str]]:\n",
        "        \"\"\"Generate synthetic training examples based on actual financial data\"\"\"\n",
        "        synthetic_examples = []\n",
        "\n",
        "        # Template patterns for synthetic data\n",
        "        templates = [\n",
        "            \"Company {company} reported quarterly revenue of ${revenue:,.0f} million with a profit margin of {profit_margin:.1f}%.\",\n",
        "            \"{company} announced financial results showing ${revenue:,.0f} million in revenue and ${net_income:,.0f} million in net income.\",\n",
        "            \"Financial highlights for {company}: Revenue ${revenue:,.0f}M, Operating margin {operating_margin:.1f}%, PE ratio {pe_ratio:.1f}.\",\n",
        "            \"{company}'s latest earnings show strong performance with ${revenue:,.0f} million revenue and {roe:.1f}% return on equity.\",\n",
        "            \"Quarterly results: {company} generated ${revenue:,.0f} million in sales with gross margin of {gross_margin:.1f}%.\"\n",
        "        ]\n",
        "\n",
        "        # Sample financial metrics for synthetic generation\n",
        "        sample_companies = [\"TechCorp\", \"InnovateCo\", \"GlobalTech\", \"DataSystems\", \"CloudTech\", \"FinanceInc\"]\n",
        "\n",
        "        for i in range(num_needed):\n",
        "            company = sample_companies[i % len(sample_companies)]\n",
        "\n",
        "            # Generate realistic financial metrics\n",
        "            revenue = np.random.uniform(1000, 50000)  # Million USD\n",
        "            profit_margin = np.random.uniform(5, 30)\n",
        "            operating_margin = np.random.uniform(8, 25)\n",
        "            gross_margin = np.random.uniform(20, 60)\n",
        "            net_income = revenue * (profit_margin / 100)\n",
        "            pe_ratio = np.random.uniform(10, 35)\n",
        "            roe = np.random.uniform(8, 25)\n",
        "\n",
        "            # Create synthetic text\n",
        "            template = templates[i % len(templates)]\n",
        "            text = template.format(\n",
        "                company=company,\n",
        "                revenue=revenue,\n",
        "                profit_margin=profit_margin,\n",
        "                operating_margin=operating_margin,\n",
        "                gross_margin=gross_margin,\n",
        "                net_income=net_income,\n",
        "                pe_ratio=pe_ratio,\n",
        "                roe=roe\n",
        "            )\n",
        "\n",
        "            # Create target metrics\n",
        "            target_metrics = {\n",
        "                \"revenue\": f\"${revenue:,.0f} million\",\n",
        "                \"profit_margin\": f\"{profit_margin:.1f}%\",\n",
        "                \"net_income\": f\"${net_income:,.0f} million\"\n",
        "            }\n",
        "\n",
        "            # Format as training example\n",
        "            user_message = f\"Extract financial metrics from the following text about {company}:\\n\\n{text}\"\n",
        "            assistant_message = json.dumps(target_metrics)\n",
        "\n",
        "            synthetic_examples.append({\n",
        "                'user': user_message,\n",
        "                'assistant': assistant_message\n",
        "            })\n",
        "\n",
        "        return synthetic_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XYtAyQC8PzkN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model architecture for financial metrics extraction\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "import logging\n",
        "from typing import Dict, List, Any\n",
        "import os\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FinancialMetricsModel:\n",
        "    \"\"\"Financial Metrics Extraction Model\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def load_model_and_tokenizer(self):\n",
        "        \"\"\"Load pre-trained model and tokenizer\"\"\"\n",
        "        print(f\"Loading model: {self.config.model.model_name}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model.model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.config.model.model_name)\n",
        "\n",
        "        # Add padding token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(\"Model and tokenizer loaded successfully\")\n",
        "\n",
        "    def prepare_datasets(self, train_data, val_data, test_data):\n",
        "        \"\"\"Prepare datasets for training\"\"\"\n",
        "        def tokenize_function(examples):\n",
        "            # Combine user and assistant messages\n",
        "            combined_texts = []\n",
        "            for user_msg, assistant_msg in zip(examples['user'], examples['assistant']):\n",
        "                combined_text = f\"{user_msg}\\n\\nAssistant: {assistant_msg}\"\n",
        "                combined_texts.append(combined_text)\n",
        "\n",
        "            # Tokenize\n",
        "            tokenized = self.tokenizer(\n",
        "                combined_texts,\n",
        "                truncation=True,\n",
        "                max_length=self.config.model.max_length,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # For language modeling, labels are the same as input_ids\n",
        "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "\n",
        "            return tokenized\n",
        "\n",
        "        # Convert to Hugging Face datasets\n",
        "        train_dataset = Dataset.from_pandas(train_data)\n",
        "        val_dataset = Dataset.from_pandas(val_data)\n",
        "        test_dataset = Dataset.from_pandas(test_data)\n",
        "\n",
        "        # Tokenize datasets\n",
        "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "        val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "        test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Setup trainer for model training\"\"\"\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.config.paths.model_checkpoints_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=self.config.model.num_epochs,\n",
        "            per_device_train_batch_size=self.config.model.batch_size,\n",
        "            per_device_eval_batch_size=self.config.model.batch_size,\n",
        "            warmup_steps=self.config.model.warmup_steps,\n",
        "            weight_decay=self.config.model.weight_decay,\n",
        "            learning_rate=self.config.model.learning_rate,\n",
        "            logging_dir=f\"{self.config.paths.output_dir}/logs\",\n",
        "            logging_steps=50,\n",
        "            save_steps=self.config.model.save_steps,\n",
        "            eval_steps=self.config.model.eval_steps,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            save_total_limit=self.config.model.save_total_limit,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            dataloader_num_workers=4,\n",
        "            prediction_loss_only=True,\n",
        "        )\n",
        "\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,  # We're doing causal language modeling\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        self.trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "        print(\"Trainer setup completed\")\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        print(\"Starting model training...\")\n",
        "\n",
        "        # Train the model\n",
        "        self.trainer.train()\n",
        "\n",
        "        # Save the final model\n",
        "        final_model_path = f\"{self.config.paths.model_dir}/final_model\"\n",
        "        self.trainer.save_model(final_model_path)\n",
        "        self.tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "        print(f\"Training completed. Model saved to {final_model_path}\")\n",
        "\n",
        "    def generate_metrics(self, text: str, max_length: int = 200) -> str:\n",
        "        \"\"\"Generate financial metrics for given text\"\"\"\n",
        "        # Prepare input\n",
        "        input_text = f\"Extract financial metrics from the following text:\\n\\n{text}\\n\\nAssistant:\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=self.config.model.max_length\n",
        "        )\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        response = self.tokenizer.decode(\n",
        "            outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8pI0wV9wP7Fg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model training orchestration\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FinancialMetricsTrainer:\n",
        "    \"\"\"Orchestrate the training process\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.model = FinancialMetricsModel(config)\n",
        "\n",
        "    def load_processed_data(self):\n",
        "        \"\"\"Load processed training data\"\"\"\n",
        "        train_data = pd.read_json(f\"{self.config.paths.processed_data_dir}/train_data.json\", lines=True)\n",
        "        val_data = pd.read_json(f\"{self.config.paths.processed_data_dir}/val_data.json\", lines=True)\n",
        "        test_data = pd.read_json(f\"{self.config.paths.processed_data_dir}/test_data.json\", lines=True)\n",
        "\n",
        "        print(f\"Loaded training data - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"Complete training pipeline\"\"\"\n",
        "        # Load data\n",
        "        train_data, val_data, test_data = self.load_processed_data()\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.model.load_model_and_tokenizer()\n",
        "\n",
        "        # Prepare datasets\n",
        "        train_dataset, val_dataset, test_dataset = self.model.prepare_datasets(\n",
        "            train_data, val_data, test_data\n",
        "        )\n",
        "\n",
        "        # Setup trainer\n",
        "        self.model.setup_trainer(train_dataset, val_dataset)\n",
        "\n",
        "        # Train model\n",
        "        self.model.train()\n",
        "\n",
        "        # Save test dataset for evaluation\n",
        "        with open(f\"{self.config.paths.processed_data_dir}/test_dataset.json\", \"w\") as f:\n",
        "            json.dump(test_data.to_dict('records'), f)\n",
        "\n",
        "        print(\"Training pipeline completed successfully\")\n",
        "\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nYbuua8WQG_P"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Evaluation metrics for financial metrics extraction\n",
        "\"\"\"\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FinancialMetricsEvaluator:\n",
        "    \"\"\"Evaluate financial metrics extraction performance\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.metric_types = [\n",
        "            'revenue', 'profit_margin', 'net_income', 'market_cap',\n",
        "            'pe_ratio', 'eps', 'gross_margin', 'operating_margin',\n",
        "            'roe', 'roa', 'debt_to_equity'\n",
        "        ]\n",
        "\n",
        "    def extract_metrics_from_json(self, json_string: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract metrics from JSON string\"\"\"\n",
        "        try:\n",
        "            return json.loads(json_string)\n",
        "        except:\n",
        "            # Fallback: try to extract metrics using regex\n",
        "            metrics = {}\n",
        "\n",
        "            # Revenue patterns\n",
        "            revenue_patterns = [\n",
        "                r'\"revenue\":\\s*\"?\\$?([\\d,]+\\.?\\d*)\"?',\n",
        "                r'\"sales\":\\s*\"?\\$?([\\d,]+\\.?\\d*)\"?'\n",
        "            ]\n",
        "\n",
        "            # Margin patterns\n",
        "            margin_patterns = [\n",
        "                r'\"(\\w*_?margin)\":\\s*\"?([\\d,]+\\.?\\d*)%?\"?',\n",
        "                r'\"(roe|roa)\":\\s*\"?([\\d,]+\\.?\\d*)%?\"?'\n",
        "            ]\n",
        "\n",
        "            # Ratio patterns\n",
        "            ratio_patterns = [\n",
        "                r'\"(pe_ratio|eps|debt_to_equity)\":\\s*\"?([\\d,]+\\.?\\d*)\"?'\n",
        "            ]\n",
        "\n",
        "            for pattern_group in [revenue_patterns, margin_patterns, ratio_patterns]:\n",
        "                for pattern in pattern_group:\n",
        "                    matches = re.findall(pattern, json_string.lower())\n",
        "                    for match in matches:\n",
        "                        if isinstance(match, tuple):\n",
        "                            key, value = match\n",
        "                            metrics[key] = value\n",
        "                        else:\n",
        "                            metrics['revenue'] = match\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    def normalize_metric_value(self, value: Any) -> float:\n",
        "        \"\"\"Normalize metric value to float\"\"\"\n",
        "        if isinstance(value, str):\n",
        "            # Remove currency symbols and commas\n",
        "            cleaned = re.sub(r'[$,%]', '', value)\n",
        "            try:\n",
        "                return float(cleaned)\n",
        "            except:\n",
        "                return 0.0\n",
        "        elif isinstance(value, (int, float)):\n",
        "            return float(value)\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_metric_accuracy(self, predicted: Dict[str, Any], actual: Dict[str, Any]) -> Dict[str, float]:\n",
        "        \"\"\"Calculate accuracy for each metric type\"\"\"\n",
        "        accuracies = {}\n",
        "\n",
        "        for metric_type in self.metric_types:\n",
        "            if metric_type in predicted and metric_type in actual:\n",
        "                pred_value = self.normalize_metric_value(predicted[metric_type])\n",
        "                actual_value = self.normalize_metric_value(actual[metric_type])\n",
        "\n",
        "                if actual_value != 0:\n",
        "                    # Calculate percentage error\n",
        "                    error = abs(pred_value - actual_value) / actual_value\n",
        "                    accuracy = max(0, 1 - error)  # Accuracy as 1 - relative error\n",
        "                    accuracies[metric_type] = accuracy\n",
        "                else:\n",
        "                    accuracies[metric_type] = 1.0 if pred_value == 0 else 0.0\n",
        "            else:\n",
        "                # Penalize missing predictions\n",
        "                accuracies[metric_type] = 0.0\n",
        "\n",
        "        return accuracies\n",
        "\n",
        "    def calculate_extraction_metrics(self, predicted: Dict[str, Any], actual: Dict[str, Any]) -> Dict[str, float]:\n",
        "        \"\"\"Calculate precision, recall, F1 for metric extraction\"\"\"\n",
        "        # Which metrics were predicted vs actually present\n",
        "        predicted_metrics = set(predicted.keys())\n",
        "        actual_metrics = set(actual.keys())\n",
        "\n",
        "        # True positives: correctly identified metrics\n",
        "        tp = len(predicted_metrics & actual_metrics)\n",
        "\n",
        "        # False positives: predicted but not actually present\n",
        "        fp = len(predicted_metrics - actual_metrics)\n",
        "\n",
        "        # False negatives: actually present but not predicted\n",
        "        fn = len(actual_metrics - predicted_metrics)\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'true_positives': tp,\n",
        "            'false_positives': fp,\n",
        "            'false_negatives': fn\n",
        "        }\n",
        "\n",
        "    def evaluate_model_predictions(self, predictions: List[str], ground_truth: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate model predictions against ground truth\"\"\"\n",
        "        results = {\n",
        "            'metric_accuracies': [],\n",
        "            'extraction_metrics': [],\n",
        "            'overall_accuracy': 0.0,\n",
        "            'detailed_results': []\n",
        "        }\n",
        "\n",
        "        for i, (pred_str, truth_str) in enumerate(zip(predictions, ground_truth)):\n",
        "            # Extract metrics from strings\n",
        "            pred_metrics = self.extract_metrics_from_json(pred_str)\n",
        "            truth_metrics = self.extract_metrics_from_json(truth_str)\n",
        "\n",
        "            # Calculate metric-specific accuracies\n",
        "            metric_accuracies = self.calculate_metric_accuracy(pred_metrics, truth_metrics)\n",
        "            results['metric_accuracies'].append(metric_accuracies)\n",
        "\n",
        "            # Calculate extraction metrics (precision, recall, F1)\n",
        "            extraction_metrics = self.calculate_extraction_metrics(pred_metrics, truth_metrics)\n",
        "            results['extraction_metrics'].append(extraction_metrics)\n",
        "\n",
        "            # Store detailed results\n",
        "            results['detailed_results'].append({\n",
        "                'index': i,\n",
        "                'predicted': pred_metrics,\n",
        "                'actual': truth_metrics,\n",
        "                'metric_accuracies': metric_accuracies,\n",
        "                'extraction_metrics': extraction_metrics\n",
        "            })\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        if results['metric_accuracies']:\n",
        "            # Average accuracy across all metrics and examples\n",
        "            all_accuracies = []\n",
        "            for acc_dict in results['metric_accuracies']:\n",
        "                all_accuracies.extend(acc_dict.values())\n",
        "            results['overall_accuracy'] = sum(all_accuracies) / len(all_accuracies) if all_accuracies else 0\n",
        "\n",
        "            # Average extraction metrics\n",
        "            avg_precision = sum(em['precision'] for em in results['extraction_metrics']) / len(results['extraction_metrics'])\n",
        "            avg_recall = sum(em['recall'] for em in results['extraction_metrics']) / len(results['extraction_metrics'])\n",
        "            avg_f1 = sum(em['f1_score'] for em in results['extraction_metrics']) / len(results['extraction_metrics'])\n",
        "\n",
        "            results['average_precision'] = avg_precision\n",
        "            results['average_recall'] = avg_recall\n",
        "            results['average_f1'] = avg_f1\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_evaluation_report(self, results: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
        "        report = []\n",
        "        report.append(\"=\"*50)\n",
        "        report.append(\"FINANCIAL METRICS EXTRACTION EVALUATION REPORT\")\n",
        "        report.append(\"=\"*50)\n",
        "\n",
        "        # Overall metrics\n",
        "        report.append(f\"\\nOVERALL PERFORMANCE:\")\n",
        "        report.append(f\"Overall Accuracy: {results['overall_accuracy']:.4f}\")\n",
        "        report.append(f\"Average Precision: {results['average_precision']:.4f}\")\n",
        "        report.append(f\"Average Recall: {results['average_recall']:.4f}\")\n",
        "        report.append(f\"Average F1-Score: {results['average_f1']:.4f}\")\n",
        "\n",
        "        # Metric-specific performance\n",
        "        if results['metric_accuracies']:\n",
        "            report.append(f\"\\nMETRIC-SPECIFIC ACCURACY:\")\n",
        "            metric_avgs = {}\n",
        "            for metric_type in self.metric_types:\n",
        "                accuracies = [ma.get(metric_type, 0) for ma in results['metric_accuracies']]\n",
        "                avg_acc = sum(accuracies) / len(accuracies) if accuracies else 0\n",
        "                metric_avgs[metric_type] = avg_acc\n",
        "                report.append(f\"{metric_type}: {avg_acc:.4f}\")\n",
        "\n",
        "        # Best and worst performers\n",
        "        if results['detailed_results']:\n",
        "            report.append(f\"\\nBEST PERFORMING EXAMPLES:\")\n",
        "            sorted_results = sorted(\n",
        "                results['detailed_results'],\n",
        "                key=lambda x: x['extraction_metrics']['f1_score'],\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            for i, result in enumerate(sorted_results[:3]):\n",
        "                report.append(f\"\\nExample {i+1} (F1: {result['extraction_metrics']['f1_score']:.4f}):\")\n",
        "                report.append(f\"Predicted: {result['predicted']}\")\n",
        "                report.append(f\"Actual: {result['actual']}\")\n",
        "\n",
        "        return \"\\n\".join(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rK_TU36sQLis"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model evaluation orchestration\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import logging\n",
        "import os\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Orchestrate model evaluation process\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.evaluator = FinancialMetricsEvaluator()\n",
        "        self.model = None\n",
        "\n",
        "    def load_trained_model(self, model_path: str = None):\n",
        "        \"\"\"Load trained model for evaluation\"\"\"\n",
        "        if model_path is None:\n",
        "            model_path = f\"{self.config.paths.model_dir}/final_model\"\n",
        "\n",
        "        self.model = FinancialMetricsModel(self.config)\n",
        "        self.model.load_model_and_tokenizer()\n",
        "\n",
        "        # Load the fine-tuned weights\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        self.model.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        self.model.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        print(f\"Loaded trained model from {model_path}\")\n",
        "\n",
        "    def load_test_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load test dataset\"\"\"\n",
        "        test_data = pd.read_json(f\"{self.config.paths.processed_data_dir}/test_data.json\", lines=True)\n",
        "        print(f\"Loaded {len(test_data)} test examples\")\n",
        "        return test_data\n",
        "\n",
        "    def generate_predictions(self, test_data: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"Generate predictions for test data\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for _, row in test_data.iterrows():\n",
        "            # Extract the input text (remove the instruction part)\n",
        "            user_text = row['user']\n",
        "            # Find the actual financial text after the instruction\n",
        "            text_start = user_text.find('\\n\\n') + 2\n",
        "            input_text = user_text[text_start:] if text_start > 1 else user_text\n",
        "\n",
        "            # Generate prediction\n",
        "            prediction = self.model.generate_metrics(input_text)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "            print(f\"Generated prediction {len(predictions)}/{len(test_data)}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate_model(self) -> Dict[str, Any]:\n",
        "        \"\"\"Complete model evaluation\"\"\"\n",
        "        # Load test data\n",
        "        test_data = self.load_test_data()\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = self.generate_predictions(test_data)\n",
        "\n",
        "        # Get ground truth\n",
        "        ground_truth = test_data['assistant'].tolist()\n",
        "\n",
        "        # Evaluate predictions\n",
        "        results = self.evaluator.evaluate_model_predictions(predictions, ground_truth)\n",
        "\n",
        "        # Generate report\n",
        "        report = self.evaluator.generate_evaluation_report(results)\n",
        "\n",
        "        # Save results\n",
        "        self.save_evaluation_results(results, report, predictions, ground_truth)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_evaluation_results(self, results: Dict[str, Any], report: str,\n",
        "                              predictions: List[str], ground_truth: List[str]):\n",
        "        \"\"\"Save evaluation results\"\"\"\n",
        "        os.makedirs(self.config.paths.output_dir, exist_ok=True)\n",
        "\n",
        "        # Save detailed results\n",
        "        with open(f\"{self.config.paths.output_dir}/evaluation_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        # Save report\n",
        "        with open(f\"{self.config.paths.output_dir}/evaluation_report.txt\", \"w\") as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Save predictions vs ground truth\n",
        "        comparison_df = pd.DataFrame({\n",
        "            'prediction': predictions,\n",
        "            'ground_truth': ground_truth\n",
        "        })\n",
        "        comparison_df.to_csv(f\"{self.config.paths.output_dir}/predictions_comparison.csv\", index=False)\n",
        "\n",
        "        print(\"Evaluation results saved\")\n",
        "\n",
        "        # Print report\n",
        "        print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z_-f8hA7QQzo",
        "outputId": "0029572b-103f-4ede-c502-e242a2874982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Financial Metrics Extraction Model Training Pipeline\n",
            "Step 1: Collecting financial data and news...\n",
            "Collecting news for Apple\n",
            "Collecting news for Microsoft\n",
            "Collecting news for Google\n",
            "Collecting news for Amazon\n",
            "Collecting news for Tesla\n",
            "Collecting news for Meta\n",
            "Collecting news for Netflix\n",
            "Collecting news for Nvidia\n",
            "Collecting news for JPMorgan\n",
            "Collecting news for Goldman Sachs\n",
            "Collecting news for Boeing\n",
            "Collecting news for Coca Cola\n",
            "Collecting news for Disney\n",
            "Collecting news for McDonald's\n",
            "Collecting news for Nike\n",
            "Collecting news for Walmart\n",
            "Collecting news for Visa\n",
            "Collecting news for Intel\n",
            "Collecting news for Adobe\n",
            "Collecting news for Salesforce\n",
            "Collected 1 articles\n",
            "Collecting financial data for Apple (AAPL)\n",
            "Collecting financial data for Microsoft (MSFT)\n",
            "Collecting financial data for Google (GOOGL)\n",
            "Collecting financial data for Amazon (AMZN)\n",
            "Collecting financial data for Tesla (TSLA)\n",
            "Collecting financial data for Meta (META)\n",
            "Collecting financial data for Netflix (NFLX)\n",
            "Collecting financial data for Nvidia (NVDA)\n",
            "Collecting financial data for JPMorgan (JPM)\n",
            "Collecting financial data for Goldman Sachs (GS)\n",
            "Collecting financial data for Boeing (BA)\n",
            "Collecting financial data for Coca Cola (KO)\n",
            "Collecting financial data for Disney (DIS)\n",
            "Collecting financial data for McDonald's (MCD)\n",
            "Collecting financial data for Nike (NKE)\n",
            "Collecting financial data for Walmart (WMT)\n",
            "Collecting financial data for Visa (V)\n",
            "Collecting financial data for Intel (INTC)\n",
            "Collecting financial data for Adobe (ADBE)\n",
            "Collecting financial data for Salesforce (CRM)\n",
            "Raw data saved successfully\n",
            "Step 2: Preprocessing data for training...\n",
            "Loaded 1 news articles and 20 financial records\n",
            "Created 1 training examples\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4139375175>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-4139375175>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Split and save data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_and_save_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversational_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Step 3: Model Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1318829357>\u001b[0m in \u001b[0;36msplit_and_save_data\u001b[0;34m(self, conversational_data)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         train_data, temp_data = train_test_split(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_split\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Main training script for financial metrics extraction model\n",
        "\"\"\"\n",
        "import os\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    print(\"Starting Financial Metrics Extraction Model Training Pipeline\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    config = Config()\n",
        "\n",
        "    # Create necessary directories\n",
        "    os.makedirs(config.paths.data_dir, exist_ok=True)\n",
        "    os.makedirs(config.paths.raw_data_dir, exist_ok=True)\n",
        "    os.makedirs(config.paths.processed_data_dir, exist_ok=True)\n",
        "    os.makedirs(config.paths.model_dir, exist_ok=True)\n",
        "    os.makedirs(config.paths.output_dir, exist_ok=True)\n",
        "\n",
        "    # Step 1: Data Collection\n",
        "    print(\"Step 1: Collecting financial data and news...\")\n",
        "    collector = FinancialDataCollector(config)\n",
        "\n",
        "    # Collect news data\n",
        "    news_data = collector.collect_news_data()\n",
        "\n",
        "    # Collect financial metrics\n",
        "    financial_data = collector.collect_financial_metrics()\n",
        "\n",
        "    # Save raw data\n",
        "    collector.save_raw_data(news_data, financial_data)\n",
        "\n",
        "    # Step 2: Data Preprocessing\n",
        "    print(\"Step 2: Preprocessing data for training...\")\n",
        "    preprocessor = FinancialDataPreprocessor(config)\n",
        "\n",
        "    # Load raw data\n",
        "    news_df, financial_df = preprocessor.load_raw_data()\n",
        "\n",
        "    # Create training examples\n",
        "    training_examples = preprocessor.create_training_examples(news_df, financial_df)\n",
        "\n",
        "    # Convert to conversational format\n",
        "    conversational_data = preprocessor.create_conversational_format(training_examples)\n",
        "\n",
        "    # Split and save data\n",
        "    train_data, val_data, test_data = preprocessor.split_and_save_data(conversational_data)\n",
        "\n",
        "    # Step 3: Model Training\n",
        "    print(\"Step 3: Training the model...\")\n",
        "    trainer = FinancialMetricsTrainer(config)\n",
        "\n",
        "    # Train model\n",
        "    trained_model = trainer.train_model()\n",
        "\n",
        "    print(\"Training pipeline completed successfully!\")\n",
        "    print(f\"Model saved to: {config.paths.model_dir}/final_model\")\n",
        "    print(f\"Training data saved to: {config.paths.processed_data_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "M0NsR1ITQ5Um",
        "outputId": "40416a7a-73f6-44c9-dd1c-d1a982a70687"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'collector' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1390664650>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'collector' is not defined"
          ]
        }
      ],
      "source": [
        "print(collector)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
