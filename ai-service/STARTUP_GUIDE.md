# ğŸš€ AI Service Startup Guide

## Quick Start

### Method 1: Using Batch Script (Recommended for Windows)
```bash
# Navigate to ai-service directory
cd ai-service

# Run the startup script
start.bat
```

### Method 2: Direct Python
```bash
# Navigate to ai-service directory
cd ai-service

# Install dependencies
pip install -r requirements.txt

# Start the service
python app.py
```

## What Happens During Startup?

When you run `python app.py`, the service will:

1. **ğŸ“± Detect Hardware**: Check if GPU is available for faster processing
2. **ğŸ—‚ï¸ Setup Cache**: Configure custom model storage paths (if set)
3. **ğŸ“¥ Download Models**: Download and cache AI models (first run only)
4. **âš¡ Load Models**: Load models into memory for fast responses
5. **ğŸŒ Start Server**: Start the Flask web service

## First Run Experience

### Expected Timeline
- **Sentiment Model**: ~2-3 minutes (500MB)
- **Summarization Model**: ~5-8 minutes (1.6GB) 
- **Text Generation Model**: ~15-25 minutes (16GB)
- **Total First Run**: 20-35 minutes

### What You'll See
```
============================================================
ğŸ¤– AI Sales Deck Generator - Model Loading
============================================================
ğŸ“± Device: GPU
ğŸ® GPU: NVIDIA GeForce RTX 4080
ğŸ’¾ GPU Memory: 16.0 GB
ğŸ—‚ï¸  Custom model cache: D:\AI_Models\huggingface

ğŸ“¥ Starting model downloads and loading...
â° This may take 10-30 minutes depending on your internet speed
ğŸ’¡ Models will be cached for future runs

ğŸ“¥ [1/3] Loading Sentiment Analysis (~500MB)...
    Model: cardiffnlp/twitter-roberta-base-sentiment-latest
    âœ… Loaded in 2.5 minutes

ğŸ“¥ [2/3] Loading Text Summarization (~1.6GB)...
    Model: facebook/bart-large-cnn
    âœ… Loaded in 5.2 minutes

ğŸ“¥ [3/3] Loading Text Generation - Llama 3.1 8B (~16GB)...
    Model: meta-llama/Llama-3.1-8B-Instruct
    âœ… Loaded in 18.3 minutes

â±ï¸  Total loading time: 25.8 minutes
âœ… All models loaded successfully!
ğŸš€ Service is ready for fast document processing

ğŸ¤– Loaded Models:
  ğŸ“ Text Generation: meta-llama/Llama-3.1-8B-Instruct
  ğŸ“„ Summarization: facebook/bart-large-cnn
  ğŸ˜Š Sentiment Analysis: cardiffnlp/twitter-roberta-base-sentiment-latest

============================================================
ğŸŒ Starting Flask server on http://localhost:5001
ğŸ¥ Health check: http://localhost:5001/health
ğŸ“Š Model progress: http://localhost:5001/model-progress

ğŸ¯ Service ready for document processing!
============================================================
```

## Subsequent Runs

After the first run, models are cached so startup is **much faster** (1-3 minutes):

```
ğŸ“¥ [1/3] Loading Sentiment Analysis (~500MB)...
    âœ… Loaded in 0.2 minutes

ğŸ“¥ [2/3] Loading Text Summarization (~1.6GB)...
    âœ… Loaded in 0.5 minutes

ğŸ“¥ [3/3] Loading Text Generation - Llama 3.1 8B (~16GB)...
    âœ… Loaded in 1.8 minutes

â±ï¸  Total loading time: 2.5 minutes
```

## Configuration Options

### Custom Model Storage
Create `.env` file:
```env
# Store models on a different drive with more space
HF_HOME=D:\AI_Models\huggingface
HUGGINGFACE_HUB_CACHE=D:\AI_Models\huggingface\hub
TRANSFORMERS_CACHE=D:\AI_Models\huggingface\transformers
```

### Skip Model Loading (Development)
```env
# Skip model loading for faster development startup
SKIP_MODEL_LOADING=true
```

## Troubleshooting

### Download Interrupted
- Models support **resume downloads**
- Simply restart `python app.py` to continue

### Insufficient Storage
- Llama 3.1 8B requires ~16GB free space
- Set custom cache location: `HF_HOME=D:\AI_Models`

### GPU Memory Issues
- Service automatically uses CPU if GPU memory insufficient
- Reduce other GPU-intensive applications

### Model Loading Fails
- Service will try fallback models (DistilGPT2)
- Check internet connection
- Verify sufficient disk space

## Service Endpoints

Once running, the service provides:

- **Main Service**: `http://localhost:5001/process-sensitive-document`
- **Health Check**: `http://localhost:5001/health`
- **Model Status**: `http://localhost:5001/model-progress`
- **Manual Reload**: `http://localhost:5001/preload-models`

## Benefits of Startup Loading

âœ… **Faster Response Times**: Models are ready immediately
âœ… **Better User Experience**: No waiting during document processing  
âœ… **Predictable Performance**: Consistent response times
âœ… **Error Detection**: Model issues caught at startup, not during use
âœ… **Progress Visibility**: See exactly what's happening during load

The trade-off is longer startup time, but much faster processing once running! 